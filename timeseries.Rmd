---
title: "Advanced Statistical Modelling (ASM) - Final Project"
subtitle: "Total production of renewable energy in USA"
author: "Odysseas Kyparissis, Pau Comas"
date: "`r Sys.Date()`"
output: html_document
---

Data is gathered from the [US Energy Information Administration](https://www.eia.gov/totalenergy/data/browser/index.php?tbl=T10.01#/?f=M&start=199001&end=201901&charted=6-7-8-9-14) and it refers to the total production of renewable energy in USA (trillions of BTU-British Thermal Units) from 1990 to 2020.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Reading and visualizing original timeseries

```{r read}
serie=window(ts(read.table("RenewUSA.dat", header=F),start=1990,freq=12))
serie
```

```{r viz}
plot(serie, main="Total production of renewable energy (USA)", ylab="Trillions of BTU-British Thermal Units")
abline(v=1990:2020,col=4,lty=3)
```

# Identification

## Determine the needed transformations to make the series stationary.

Justify the transformations carried out using graphical and numerical results.

**Is Variance constant?**

```{r}
boxplot(serie~floor(time(serie)), ylab="Trillions of BTU-British Thermal Units")
```


```{r}
m<-apply(matrix(serie,nrow=12),2,mean)
v<-apply(matrix(serie,nrow=12),2,var)
plot(v~m)
abline(lm(v~m),col=2,lty=3)
```

Although it's not perfectly clear from the previous two figures, once the mean values get higher the variance get higher as well. For that reason, it is decided to use the logarithmic transformation of the original series.

```{r}
lnserie=log(serie)
plot(lnserie, ylab="Logarithm of Trillions of BTU-British Thermal Units")
abline(v=1990:2020,col=4,lty=3)
abline(h=0)
```


```{r}
matrixlnserie <- matrix(lnserie,nrow=12)
boxplot(matrixlnserie)
```
```{r}
m<-apply(matrixlnserie,2,mean)
v<-apply(matrixlnserie,2,var)
plot(v~m)
abline(lm(v~m),col=2,lty=3)
```

Now, it can be seen that we erased the raise of variability over the mean successfully.

**Is seasonality present?**


```{r}
plot(decompose(lnserie))
```

We can clearly see a potential seasonal component, we'll confirm it nextly. log(Xt) = Linear Trend + Seasonal component + stationary process

```{r}
monthplot(lnserie)
```
```{r}
ts.plot(matrix(lnserie,nrow=12),col=1:8)
```

It can be seen that there is an indication about a seasonal pattern, since the mean values of different months fluclutate, and the lines of the timeserie grouped by month show some parallel effects. For this reason a seasonal difference is applied.

- In September we usually have the lowest renewable energy production, and in May the highest.

```{r}
d12lnserie<-diff(lnserie,lag=12)
plot(d12lnserie)
abline(h=0)
abline(h=mean(d12lnserie),col=2)
```
```{r}
monthplot(d12lnserie)
```
```{r}
ts.plot(matrix(d12lnserie,nrow=12),col=1:8)
```

Now it is clear that the seasonal pattern is removed.

**Is the mean constant?**

We have to apply one regular difference trying to make the mean constant:

```{r}
d1d12lnserie=diff(d12lnserie)
plot(d1d12lnserie)
abline(h=0)
abline(h=mean(d1d12lnserie),col=2)
```

Mean seemingly constant equal to zero!!!

**Check for over-differentiation: Take an extra differentiation and compare the variances.**

```{r}
d1d1d12lnserie=diff(d1d12lnserie)
plot(d1d1d12lnserie)
abline(h=0)
abline(h=mean(d1d1d12lnserie),col=2)
```


```{r}
var(serie)
var(lnserie)
var(d12lnserie)
var(d1d12lnserie)
var(d1d1d12lnserie)

```
`d1d12lnserie` presents the lower variance, so we take it for future steps.

**Is the current series already stationary? Plot the ACF and decide!!**

```{r}
par(mfrow=c(1,2))
acf(d1d12lnserie, ylim=c(-1,1), lag.max=60, col=c(2,rep(1,11)), main ="ACF(serie)", lwd=2)
pacf(d1d12lnserie, ylim=c(-1,1), lag.max=60, col=c(rep(1,11),2), main ="PACF(serie)", lwd=2)
```

* Under Stationarity: ACF falls immediately from 1 to 0
* Under Non-stationary: the ACF declines gradually from 1 to 0 over a prolonged period of time

Thus, we can conclude that `d1d12lnserie` is a stationary series.

Final transformation of the total production of renewable energy (USA) timeseries:
$W_t = (1 - B)(1 - B^{12})\log X_t$

## Analyze the ACF and PACF of the stationary series to identify at least two plausible models. 
Reason about what features of the correlograms you use to identify these models.

* Seasonal part (red lags): MA(Q=1) or AR(P=4). Since MA(Q=1) has only a single parameter it is preferred.
  * (0,1,1)_12 (MA(Q=1))
  * (4,1,0)_12 (AR(P=4))
* Regular part (black lags): MA(q=2) (considering that black lags around 2nd red lag to be satellites not taking them into account) or AR(p=3) (again considering that black lags around 2nd red lag to be satellites not taking them into account). Since AR(p=3) has 3 parameters we can try with ARMA(1,1) as well.
  * (0,1,2) (MA(q=2))
  * (3,1,0) (AR(p=3))
  * (1,1,1) (ARMA(p=1,q=1))

### Final 6 proposed models:
* (0,1,2)(0,1,1)_12
* (3,1,0)(0,1,1)_12
* (1,1,1)(0,1,1)_12
* (0,1,2)(4,1,0)_12
* (3,1,0)(4,1,0)_12
* (1,1,1)(4,1,0)_12

# Estimation

## Use R to estimate the identified models.

```{r}
plot(d1d12lnserie)
abline(h=0) # black at 0
abline(h=mean(d1d12lnserie), col=2) # red close to 0
```


```{r}
(mod0a=arima(d1d12lnserie,order=c(0,0,2),seasonal=list(order=c(0,0,1),period=12)))
```

```{r}
(mod0b=arima(d1d12lnserie,order=c(3,0,0),seasonal=list(order=c(0,0,1),period=12)))
```


```{r}
(mod0c=arima(d1d12lnserie,order=c(1,0,1),seasonal=list(order=c(0,0,1),period=12)))
```

```{r}
(mod0d=arima(d1d12lnserie,order=c(0,0,2),seasonal=list(order=c(4,0,0),period=12)))
```

```{r}
(mod0e=arima(d1d12lnserie,order=c(3,0,0),seasonal=list(order=c(4,0,0),period=12)))
```

```{r}
(mod0f=arima(d1d12lnserie,order=c(1,0,1),seasonal=list(order=c(4,0,0),period=12)))
```
```{r include=FALSE}
# abs(mod$coef/sqrt(diag(mod$var.coef)))
# cat("\nSignificant?:",abs(model$coef/sqrt(diag(model$var.coef)))>2)
```


In all cases, the t-ratio of the intercept is lower than 2, for example:

* `mod0a` and `mod0c`:

  * \[ \frac{1e-04}{2e-04} = 0.5 < 2 \]

The same is true for the rest of the models following the same idea, meaning that the intercept is not significant. Since the mean was very close to 0 the result was expected. Thus, we can discard the mean, meaning we can move to the training of the model with the original format of the data (after the application of the logarithm).

## Estimating parameters and checking their significance

```{r}
(mod1a <- arima(lnserie, order=c(0,1,2), seasonal=list(order=c(0,1,1), period=12)))
```
AIC decreased when the intercept was dropped comparing to previous model; thus it was a good decision to drop the intercept.

Let's check the significance of the parameters of `mod1a`:
ma1: \[
\frac{0.2486}{0.0537} = 4.629423 > 2
\]
ma2: \[
\frac{0.2256}{0.0558} = 4.043011 > 2
\]
sma1: \[
\frac{0.8317}{0.0437} = 19.03204 > 2
\]

Thus we can see that all parameters are significant, no need to fix any of them.

Let's check the significance of the parameters of `mod1b` now:

```{r}
(mod1b=arima(lnserie,order=c(3,1,0),seasonal=list(order=c(0,1,1),period=12)))
```
ar1: \[
\frac{0.2531}{0.0550} = 4.601818 > 2
\]
ar2: \[
\frac{0.2808}{0.0541} = 5.190388 > 2
\]
ar3: \[
\frac{0.0982}{0.0542} = 1.811808 < 2
\]
sma1: \[
\frac{0.8163}{0.0445} = 18.93409 > 2
\]

we can see that `ar3` is not significant, thus we can retrain the model by downgrading the AR part to have `p=2`.


```{r}
(mod1b_2=arima(lnserie,order=c(2,1,0),seasonal=list(order=c(0,1,1),period=12)))
```

ar1: \[
\frac{0.2287}{0.0535} = 4.274766 > 2
\]
ar2: \[
\frac{0.2581}{0.0528} = 4.888258 > 2
\]
sma1: \[
\frac{0.8123}{0.0439} = 18.50342 > 2
\]

Now all parameters are significant, athough the `AIC` measure got higher, which means that although we could see `ar3` was deemed non-significant, the model with it got a slightly better fit measure and gives us an argument to keep `mod1b`.

Let's check the significance of the parameters of `mod1c` now:

```{r}
(mod1c <- arima(lnserie, order=c(1,1,1),seasonal=list(order=c(0,1,1),period=12)))
```
ar1: \[
\frac{0.4597}{0.1157} = 3.973207 > 2
\]
ma1: \[
\frac{0.7434}{0.0872} = 8.525229 > 2
\]
sma1: \[
\frac{0.8331}{0.0440} = 18.93409 > 2
\]

again all of them are significant.

Let's check the significance of the parameters of `mod1d` now:

```{r}
(mod1d=arima(lnserie,order=c(0,1,2),seasonal=list(order=c(4,1,0),period=12)))
```

ma1: \[
\frac{0.2537}{0.0548} = 4.629562 > 2
\]
ma2: \[
\frac{0.2135}{0.0550} = 3.881818 > 2
\]
sar1: \[
\frac{0.625}{0.054} = 11.57407 > 2
\]
sar2: \[
\frac{0.5160}{0.0627} = 8.229665 > 2
\]
sar3: \[
\frac{0.3151}{0.0627} = 5.025518 > 2
\]
sar4: \[
\frac{0.2303}{0.0557} = 4.13465 > 2
\]

For `mod1e`:

```{r}
(mod1e=arima(lnserie,order=c(3,1,0),seasonal=list(order=c(4,1,0),period=12)))
```

ar1: \[
\frac{0.2679}{0.0615} = 4.356098 > 2
\]
ar2: \[
\frac{0.2949}{0.0786} = 3.751908 > 2
\]
ar3: \[
\frac{0.0972}{0.0602} = 1.614618 < 2
\]
sar1: \[
\frac{0.6291}{0.0632} = 9.954114 > 2
\]
sar2: \[
\frac{0.5229}{0.0888} = 5.888514 > 2
\]
sar3: \[
\frac{0.3126}{0.0751} = 4.16245 > 2
\]
sar4: \[
\frac{0.2222}{0.0848} = 2.620283 > 2
\]

Again, it can be seen that `ar3` is an insignificant parameter, for that reason we will change it to an `AR` with `p=2`.

```{r}
(mod1e_2=arima(lnserie,order=c(2,1,0),seasonal=list(order=c(4,1,0),period=12)))
```

ar1: \[
\frac{0.2417}{0.0538} = 4.492565 > 2
\]
ar2: \[
\frac{0.2706}{0.0530} = 5.10566 > 2
\]
sar1: \[
\frac{0.6342}{0.0544} = 11.65809 > 2
\]
sar2: \[
\frac{0.5209}{0.0632} = 8.242089 > 2
\]
sar3: \[
\frac{0.3149}{0.0631} = 4.990491 > 2
\]
sar4: \[
\frac{0.2247}{0.0558} = 4.026882 > 2
\]

For `mod1f`:

```{r}
(mod1f=arima(lnserie,order=c(1,1,1),seasonal=list(order=c(4,1,0),period=12)))

```

Again:

ar1: \[
\frac{0.4016}{0.1315} = 3.053992 > 2
\]
ma1: \[
\frac{0.6974}{0.1033} = 6.75121 > 2
\]
sar1: \[
\frac{0.6255}{0.0542} = 11.54059 > 2
\]
sar2: \[
\frac{0.5100}{0.0630} = 8.095238 > 2
\]
sar3: \[
\frac{0.3100}{0.0628} = 4.936306 > 2
\]
sar4: \[
\frac{0.2269}{0.0558} = 4.066308 > 2
\]


To conclude, we can see that between all models the lowest AIC values is $AIC = -1230.79$ for `mod1a`, followed by `mod1b` with $AIC = -1228.93$. For this reason it is decided to continue the analysis with `mod1a` and `mod1b`.

# Validation

```{r include=FALSE}
#################Validation#################################
validation=function(model){
  s=frequency(get(model$series))
  resi=model$residuals
  par(mfrow=c(2,2),mar=c(3,3,3,3))
  #Residuals plot
  plot(resi,main="Residuals")
  abline(h=0)
  abline(h=c(-3*sd(resi),3*sd(resi)),lty=3,col=4)
  #Square Root of absolute values of residuals (Homocedasticity)
  scatter.smooth(sqrt(abs(resi)),main="Square Root of Absolute residuals",
                 lpars=list(col=2))
  
  #Normal plot of residuals
  qqnorm(resi)
  qqline(resi,col=2,lwd=2)
  
  ##Histogram of residuals with normal curve
  hist(resi,breaks=20,freq=FALSE)
  curve(dnorm(x,mean=mean(resi),sd=sd(resi)),col=2,add=T)
  
  
  #ACF & PACF of residuals
  par(mfrow=c(1,2))
  acf(resi,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=2)
  pacf(resi,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=2)
  par(mfrow=c(1,1))
  
  #Ljung-Box p-values
  par(mar=c(2,2,1,1))
  tsdiag(model,gof.lag=7*s)
  cat("\n--------------------------------------------------------------------\n")
  print(model)
  
  #Stationary and Invertible
  cat("\nModul of AR Characteristic polynomial Roots: ", 
      Mod(polyroot(c(1,-model$model$phi))),"\n")
  cat("\nModul of MA Characteristic polynomial Roots: ",
      Mod(polyroot(c(1,model$model$theta))),"\n")
  
  suppressMessages(require(forecast,quietly=TRUE,warn.conflicts=FALSE))
  plot(model)
  
  #Model expressed as an MA infinity (psi-weights)
  psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
  names(psis)=paste("psi",1:36)
  cat("\nPsi-weights (MA(inf))\n")
  cat("\n--------------------\n")
  print(psis[1:24])
  
  #Model expressed as an AR infinity (pi-weights)
  pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
  names(pis)=paste("pi",1:36)
  cat("\nPi-weights (AR(inf))\n")
  cat("\n--------------------\n")
  print(pis[1:24])
   
  cat("\nDescriptive Statistics for the Residuals\n")
  cat("\n----------------------------------------\n") 
  
  suppressMessages(require(fBasics,quietly=TRUE,warn.conflicts=FALSE))
  ##Anderson-Darling test
  print(basicStats(resi))
  
  ## Add here complementary tests (use with caution!)
  ##---------------------------------------------------------
  cat("\nNormality Tests\n")
  cat("\n--------------------\n")
 
  ##Shapiro-Wilks Normality test
  print(shapiro.test(resi))

  suppressMessages(require(nortest,quietly=TRUE,warn.conflicts=FALSE))
  ##Anderson-Darling test
  print(ad.test(resi))
  
  suppressMessages(require(tseries,quietly=TRUE,warn.conflicts=FALSE))
  ##Jarque-Bera test
  print(jarque.bera.test(resi))
  
  cat("\nHomoscedasticity Test\n")
  cat("\n--------------------\n")
  suppressMessages(require(lmtest,quietly=TRUE,warn.conflicts=FALSE))
  ##Breusch-Pagan test
  obs=get(model$series)
  print(bptest(resi~I(obs-resi)))
  
  cat("\nIndependence Tests\n")
  cat("\n--------------------\n")
  
  ##Durbin-Watson test
  print(dwtest(resi~I(1:length(resi))))
  
  ##Ljung-Box test
  cat("\nLjung-Box test\n")
  print(t(apply(matrix(c(1:4,(1:4)*s)),1,function(el) {
    te=Box.test(resi,type="Ljung-Box",lag=el)
    c(lag=(te$parameter),statistic=te$statistic[[1]],p.value=te$p.value)})))
  
}
################# Fi Validation #################################
```

## mod1a

### Residual Plot

In the following plot we need to see a constant variance of the residuals. Between the blue lines of -2, +2 stds (for each thousand only 2 can be outside) I need to find the 95% of the observations. Or 97.5% between -3, +3 stds (for each thousand only 3 can be outside).

```{r}
res = resid(mod1a)
plot(res, type='l')
abline(h=0)
abline(h=c(-3,-2,2,3)*sd(res), lty=3, col=4)
```

We can see that there are several cases where the data lay outside the intervals of standard deviations violating the afore-mentioned rule. Thus, either `clusters of volatility` exist, meaning that the variance increases (or in general changes a lot) in specific periods, or `outliers` are present which need to be treated. Outlier treatment is performed lately.


### Estimation of the Dispertion

```{r}
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Residuals plot
plot(res,main="Residuals")
abline(h=0)
abline(h=c(-3*sd(res),3*sd(res)),lty=3,col=4)
#Square Root of absolute values of residuals (Homocedasticity)
scatter.smooth(sqrt(abs(res)),main="Square Root of Absolute residuals",
lpars=list(col=2))
```

We can see in both graphical representations generated before, that the variance of the residuals is not constant. The red line shows that there is bigger variance during a specific period.

```{r}
par(mfrow=c(1,2))
acf(res^2, ylim=c(-1,1), lag.max = 72, col=c(2,rep(1,11)),lwd=2)
pacf(res^2, ylim=c(-1,1), lag.max = 72, col=c(rep(1,11),2), lwd=2)
par(mfrow=c(1,1))
```

We need all the lags to be not significant, meaning that there is no correlation between the residuals with them-selfesWhen we find significant lags here, implies that there is volatility. It means that we have a time series that at some point the variability is small and at the same level in a different time the variability is high.

When heteroscedasticity is present, there are 2 things that might lead to this result, or we have outliers or volatility. If the points lying outside the borders are not so much most probably we are having outliers, otherwise we might have volatility.


### Normality test

```{r}
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Normal plot of residuals
qqnorm(res)
qqline(res,col=2,lwd=2)
##Histogram of residuals with normal curve
hist(res,breaks=10,freq=F)
curve(dnorm(x,mean=mean(res),sd=sd(res)),col=2,add=T)

##Shapiro-Wilks Normality test
print(shapiro.test(res))
```

In the Q-Q plot we need to find the residuals lying near the red line. If we find curvature in the left plot, it represents asymmetry of the residuals. For example, in economic data we could find in the start and the end big differences from the red lines. If those points are a lot it might indicate volatility (HEAVY TAILS) (Excess of kurtosis) (this means that the kurtosis, 3rd moment, is greater than 3, that the probability of outliers is greater than the Gaussian), if they are not a lot, they could represent outliers. 

In this case, according to the Q-Q plot and the histogram, we believe normality could be reasonably acceptable, but with the presence of outliers. We base this in the countable number of points that diverge at the ends of the red line in the Q-Q plot. Because of this, the Shapiro-Wilk test rejects normality by a small margin, which we credit to the outliers effect.

### Total Validation

```{r}
validation(mod1a)
```

With the initial analyses, we have rejected homoscedasticity and reasonably accepted normality. Now we evaluate invertibility, causality and independence with the remaining analyses:

**Invertibility and causality**

If all the complex roots of the characteristic polynomial of the `MA` part lie outside the unit circle the model is invertible. Here, we deal with the inverse of the roots, so we want all the points inside instead of outside, which is true. Thus, the model can be represented as a convergent $AR(\infty)$ expression with $\pi$-weights (useful for estimating point predictions). For causality, we know that all MA(q) models are causal, which is the case. Thus, the model can be represented as a convergent $MA(\infty)$ expression with $\psi$-weights(useful for estimating the variance of estimated point predictions).

**Residuals independence**

As for `ljung box` tests, we see several rejections of white noise for residuals, since there are p values lower than $0,05$. The null hypothesis for the Ljung-Box test is that the auto-correlations of a time series are zero for all lags up to a certain specified lag $k$. In this specific case, from lag 22 to beyond, most of the lags fail the test for no-autocorrelation, so it can't be deemed as white noise behavior.

## mod1b

In this case, we won't go step by step and will perform the whole validation with the `validation` function graphs and tests at once.

```{r}
validation(mod1b)

res2 = resid(mod1b)
par(mfrow=c(1,2))
acf(res2^2, ylim=c(-1,1), lag.max = 72, col=c(2,rep(1,11)),lwd=2)
pacf(res2^2, ylim=c(-1,1), lag.max = 72, col=c(rep(1,11),2), lwd=2)
par(mfrow=c(1,1))
```

**Normality and variance**

We have very similar behavior as in `mod1a`, with a possible normality affected by outliers, according to the Q-Q plot and the histogram. Nonetheless, for this model, the Shapiro-Wilk test acknowledges normality, as opposite to the other model. For variance, outliers could also be making the Breusch-Pagan test to reject homoscedasticity and the square root of absolute residuals shows a bit of variance variability at the first half of the data.

**Invertibility and causality**

Since --> Modul of AR Characteristic polynomial Roots:  1.814813 1.814813 3.090462 and --> Modul of MA Characteristic polynomial Roots:  1.017058 1.017058 1.017058 1.017058 1.017058 1.017058 1.017058 1.017058 1.017058 1.017058 1.017058 1.017058 

We can say the model is invertible and causal. Thus, the model can be represented as a convergent $AR(\infty)$ expression with $\pi$-weights (useful for estimating point predictions) In addition, the model can be represented as a convergent $MA(\infty)$ expression with $\psi$-weights(useful for estimating the variance of estimated point predictions).

**Residuals independence**

As for `ljung box` tests, we see several rejections of white noise for residuals, since there are p values lower than $0,05$. The null hypothesis for the Ljung-Box test is that the auto-correlations of a time series are zero for all lags up to a certain specified lag $k$. In this specific case, from lag 30 to beyond, most of the lags fail the test for no-autocorrelation, so it can't be deemed as white noise behavior.


# Predictions

Both models showcase similar behavior affected potentially by outliers. Nonetheless, slightly more favorable assessments obtained with `mod1b` for normality and Ljung-Box test, and very similar `AIC` goodness of fit and simplicity indicator is the reason why we choose `mod1b` for the prediction section.

We use proposed `mod1b` $ARIMA(3,1,0)(0,1,1)_{12}$ model for predictions

## Stability of mod1b

```{r}
# capability of prediction and calculation of the measures
# position of last observation for train dataset is 12 2018. test from there until 2019 12. 
ultim=c(2018,12)
pdq=c(3,1,0)
PDQ=c(0,1,1)

serie2=window(serie, end=ultim)
lnserie2=log(serie2)
serie1=window(serie, end=ultim+c(1,0))
lnserie1=log(serie1)

(modA=arima(lnserie1,order=pdq,seasonal=list(order=PDQ,period=12)))

#we want to check if model with train dataset is very similar to model with whole set. this is to check if test set is stable. 
# we check magnitude, t ratio and signs.
# if they were not similar, model is not stable, and means i cannot calculate with metrics the model capability, structural change in test period. 


```

```{r}
(modB=arima(lnserie2,order=pdq,seasonal=list(order=PDQ,period=12)))
```
Clearly, stability of the model is fulfilled. We observe similar results in terms of significance, sign and magnitude. In practice, this means that the correlation structure has not changed in the last year, and that the use of the complete series for making predictions is reliable. Test set is stable.

## Forecasting


```{r}
pred=predict(modB,n.ahead=12)
pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)
se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)


ts.plot(serie,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-2,+2),type="o",main=paste("Model ARIMA(",paste(pdq,collapse=","),")(",paste(PDQ,collapse=","),")12",sep=""))
abline(v=(ultim[1]-2):(ultim[1]+2),lty=3,col=4)
```
```{r}
(previs=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))

```



```{r}
obs=window(serie,start=ultim+c(0,1))
pr=window(pr,start=ultim+c(0,1))
ts(data.frame(LowLim=tl[-1],Predic=pr,UpperLim=tu[-1],Observ=obs,Error=obs-pr,PercentError=(obs-pr)/obs),start=ultim+c(0,1),freq=12)
```
```{r}
mod.RMSE1=sqrt(sum((obs-pr)^2)/12)
mod.MAE1=sum(abs(obs-pr))/12
mod.RMSPE1=sqrt(sum(((obs-pr)/obs)^2)/12)
mod.MAPE1=sum(abs(obs-pr)/obs)/12

data.frame("RMSE"=mod.RMSE1,"MAE"=mod.MAE1,"RMSPE"=mod.RMSPE1,"MAPE"=mod.MAPE1)
```
```{r}
mCI1=mean(tu-tl)

cat("\nMean Length CI: ",mCI1)
```
#### erform long term predictions

Predict values for after 2020 based on the complete series. We use our model with the whole set as training set --> `modA`
```{r}
pred=predict(modA,n.ahead=12)
pr<-ts(c(tail(lnserie1,1),pred$pred),start=ultim+c(1,0),freq=12)
se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

tl1<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu1<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr1<-ts(exp(pr),start=ultim+c(1,0),freq=12)

ts.plot(serie,tl1,tu1,pr1,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=c(ultim[1]-2,ultim[1]+3),type="o",main=paste("Model ARIMA(",paste(pdq,collapse=","),")(",paste(PDQ,collapse=","),")12",sep=""))
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)
```
```{r}
(previs1=window(cbind(tl1,pr1,tu1),start=ultim+c(1,0)))
```

# Outlier Treatment

## Calendar effects

Because we have a monthly series, we check the significance of the potential calendar effects on our log_transformed series, which will be Easter and Trading Days over our model `mod1a`

```{r}
source("CalendarEffects.r")
data=c(start(lnserie)[1],start(lnserie)[2], length(lnserie))

(wTradDays=Wtrad(data))
(wEast=Weaster(data))
```

```{r}
(mod1b)
```
```{r}
(mod1a_TD <- arima(lnserie, order=c(3,1,0), seasonal=list(order=c(0,1,1), period=12),xreg=wTradDays))
```

```{r}
(mod1a_E <- arima(lnserie, order=c(3,1,0), seasonal=list(order=c(0,1,1), period=12),xreg=wEast))
```


```{r}
(mod1a_ETD <- arima(lnserie, order=c(3,1,0), seasonal=list(order=c(0,1,1), period=12), xreg=data.frame(wTradDays,wEast)))
```
We can observe no significance of any of the new coefficients based on calendar effects, each one has t_ratio < 2 and their inclusion doesn't return any improvement whatsoever in the AIC. In other terms, the calendar effects are determined to be NOT significant in this time series.



## Outliers automatic detection and its treatment

```{r}
##Detection of outliers: In this case, we have applied a regular and a seasonal differentiation of order $S=12$. We set the criterion to $crit = 2.8$ and also the argument LS to TRUE.
## The crit value chosen by the researcher is typically fixed around 3; the LS argument is optional (= TRUE if one aims to detect a level shift)

#Our final model from the previous part was modA, which now will be called for simplicity mod
mod = modA

source("atipics2.r")

mod.atip=outdetec(mod,dif=c(1,12),crit=2.8,LS=T) # automatic detection of outliers with crit=2.8 and LS =TRUE

#Estimated residual variance after outliers detection and treatment
mod.atip$sigma

atipics=mod.atip$atip[order(mod.atip$atip[,1]),]
meses=c("Ene","Feb","Mar","Abr","May","Jun","Jul","Ago","Sep","Oct","Nov","Dic")

data.frame(atipics,Fecha=paste(meses[(atipics[,1]-1)%%12+1],start(lnserie)[1]+((atipics[,1]-1)%/%12)),perc.Obs=exp(atipics[,3])*100)
```

Interpretation of some of the outliers found:

- The 14th observation is identified as a transient change (TC) outlier, characterized by a notable statistical value |3.6|>2. In the logarithmic scale (our series being log-transformed), its magnitude is represented by Wcoeff=-0.12, indicating a decline in the number of passengers compared to what would have occurred without this atypical event. The impact of the TC outlier is evident in February 1991 during the Iraq War but diminishes relatively swiftly over subsequent periods, following an exponential decrease with a delta value of 0.7.

- On January 2001, a level shift (LS) outlier with -0.12 W_coeff is found, which means that from that point on the renewable production was lower than expected. After researching, the Department of Energy (DOE) of the US attributed much of the decline to a drought that cut generation of hydroelectric power by 23 percent, but as we can see that outlier is of type LS and not a circumstantial issue, which contradicts the DOE's version.

- From June 1992 on, the renewable energy production raised a 10% (level shift outlier). On October of that year, the Energy Policy Act of 1992 (EPAct 1992) provided three types of incentives to support the development of renewable energy resources. Although the law approval and the trend don't coincide in the month, we believe it could be the cause.


### Obtaining linearized time series and visualizing it

We can see below on red the linealized time series and in black the original time series.

```{r}
lnserie.lin=lineal(lnserie,mod.atip$atip)
serie.lin=exp(lnserie.lin)

plot(serie.lin,col=2)
lines(serie)
```

### Profile of outliers effect: plot of the outliers effect in the log-transformed series
```{r}
plot(lnserie-lnserie.lin)
```

## Identification and Estimation based on the Linearized Series

We apply a regular difference and a seasonal difference (12 months) and we can propose a model for the linearized version: 

```{r}
d1d12lnserie.lin=diff(diff(lnserie.lin,12))
par(mfrow=c(1,2))
acf(d1d12lnserie.lin,ylim=c(-1,1),lag.max=72,col=c(2,rep(1,11)),lwd=2)
pacf(d1d12lnserie.lin,ylim=c(-1,1),lag.max=72,col=c(rep(1,11),2),lwd=2)
par(mfrow=c(1,1))
```
* Seasonal part (red lags): SMA(Q=1) pretty clearly
  * (0,1,1)_12 (SMA(Q=1))
* Regular part (black lags): MA(q=2) or AR(p=3). Since AR(p=3) has 3 parameters we can try with ARMA(1,1) as well. 
  * (0,1,2) (MA(q=2))
  * (3,1,0) (AR(p=3))
  * (1,1,1) (ARMA(p=1,q=1))

### Final 3 proposed models:
* (0,1,2)(0,1,1)_12
* (3,1,0)(0,1,1)_12
* (1,1,1)(0,1,1)_12


```{r}
(mod.lin.a=arima(lnserie.lin,order=c(0,1,2),seasonal=list(order=c(0,1,1),period=12)))

abs(mod.lin.a$coef/sqrt(diag(mod.lin.a$var.coef)))
cat("\nSignificant?:",abs(mod.lin.a$coef/sqrt(diag(mod.lin.a$var.coef)))>2)
```

```{r}
(mod.lin.b=arima(lnserie.lin,order=c(3,1,0),seasonal=list(order=c(0,1,1),period=12)))

abs(mod.lin.a$coef/sqrt(diag(mod.lin.a$var.coef)))
cat("\nSignificant?:",abs(mod.lin.a$coef/sqrt(diag(mod.lin.a$var.coef)))>2)
```

```{r}
(mod.lin.b=arima(lnserie.lin,order=c(1,1,1),seasonal=list(order=c(0,1,1),period=12)))

abs(mod.lin.a$coef/sqrt(diag(mod.lin.a$var.coef)))
cat("\nSignificant?:",abs(mod.lin.a$coef/sqrt(diag(mod.lin.a$var.coef)))>2)
```
We see that as in the non-linearized case, the best models according to the AIC indicator are A = (0,1,2)(0,1,1)_12 and B = (3,1,0)(0,1,1)_12. Before, the validation process made us decide in favour of model B though having slightly worse AIC but slightly better results after the residual analysis. Because the AIC differences are still small here, we will decide to use `mod.lin.b` for validation and forecasting comparison to its rival `mod1b`

## Validation of model fitted to the (log) linearized series

```{r}
model=mod.lin.b       #Fitted ARIMA model: in this case mod is ARIMA(0,1,2)(0,1,1)_12 for lnserie
validation(model)
```
**Normality and variance**
They are both reasonably accepted according to Shapiro-Wilk and Breusch-Pagan tests, and by Q-Q plots, histogram, square root of residuals. Residuals plot doesn't have outliers out of confidence interval, which it had in previous models.


**Invertibility and causality**
Yes to both, all roots >1 in both cases for AR(inf) and MA(inf)


**Residuals independence**

Failed according to Ljung-Box statistic from a certain lag >15 and so on. Nonetheless, Durbin-Watson doesn't reject autocorrelation <0.


## Forecasting and prediction capability: linearized vs original model


