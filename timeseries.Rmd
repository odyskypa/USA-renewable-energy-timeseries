---
title: "Advanced Statistical Modelling (ASM) - Final Project"
subtitle: "Total production of renewable energy in USA"
author: "Odysseas Kyparissis, Pau Comas"
date: "`r Sys.Date()`"
output: html_document
---

Data is gathered from the [US Energy Information Administration](https://www.eia.gov/totalenergy/data/browser/index.php?tbl=T10.01#/?f=M&start=199001&end=201901&charted=6-7-8-9-14) and it refers to the total production of renewable energy in USA (trillions of BTU-British Thermal Units) from 1990 to 2020.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Reading and visualizing original timeseries

```{r read}
serie=window(ts(read.table("RenewUSA.dat", header=F),start=1990,freq=12))
serie
```

```{r viz}
plot(serie, main="Total production of renewable energy (USA)", ylab="Trillions of BTU-British Thermal Units")
abline(v=1990:2020,col=4,lty=3)
```

# Identification

## Determine the needed transformations to make the series stationary.

Justify the transformations carried out using graphical and numerical results.

**Is Variance constant?**

```{r}
boxplot(serie~floor(time(serie)), ylab="Trillions of BTU-British Thermal Units")
```


```{r}
m<-apply(matrix(serie,nrow=12),2,mean)
v<-apply(matrix(serie,nrow=12),2,var)
plot(v~m)
abline(lm(v~m),col=2,lty=3)
```

Although it's not perfectly clear from the previous two figures, once the mean values get higher the variance get higher as well. For that reason, it is decided to use the logarithmic transformation of the original series.

```{r}
lnserie=log(serie)
plot(lnserie, ylab="Logarithm of Trillions of BTU-British Thermal Units")
abline(v=1990:2020,col=4,lty=3)
abline(h=0)
```


```{r}
matrixlnserie <- matrix(lnserie,nrow=12)
boxplot(matrixlnserie)
```
```{r}
m<-apply(matrixlnserie,2,mean)
v<-apply(matrixlnserie,2,var)
plot(v~m)
abline(lm(v~m),col=2,lty=3)
```

Now, it can be seen that the difference of variability is smaller we go to higher values.

**Is seasonality present?**


```{r}
plot(decompose(lnserie))
```

log(Xt) = Linear Trend + Seasonal component + stationary process

```{r}
monthplot(lnserie)
```
```{r}
ts.plot(matrix(lnserie,nrow=12),col=1:8)
```

It can be seen that there is an indication about a seasonal pattern, since the mean values of different months
fluclutate, and the lines of the timeserie grouped by month show some parallel effects. For this reason a seasonal difference is applied.

```{r}
d12lnserie<-diff(lnserie,lag=12)
plot(d12lnserie)
abline(h=0)
abline(h=mean(d12lnserie),col=2)
```
```{r}
monthplot(d12lnserie)
```
```{r}
ts.plot(matrix(d12lnserie,nrow=12),col=1:8)
```

Now it is clear that the seasonal pattern is removed.

**Is the mean constant?**

We have to apply one regular difference trying to make the mean constant:

```{r}
d1d12lnserie=diff(d12lnserie)
plot(d1d12lnserie)
abline(h=0)
abline(h=mean(d1d12lnserie),col=2)
```

Mean seemingly constant equal to zero!!!

**Check for over-differentiation: Take an extra differentiation and compare the variances.**

```{r}
d1d1d12lnserie=diff(d1d12lnserie)
plot(d1d1d12lnserie)
abline(h=0)
abline(h=mean(d1d1d12lnserie),col=2)
```


```{r}
var(serie)
```

```{r}
var(lnserie)
```

```{r}
var(d12lnserie)
```
```{r}
var(d1d12lnserie)
```
```{r}
var(d1d1d12lnserie)
```
`d1d12lnserie` presents the lower variance.

**Is the current series already stationary? Plot the ACF and decide!!**

```{r}
par(mfrow=c(1,2))
acf(d1d12lnserie, ylim=c(-1,1), lag.max=60, col=c(2,rep(1,11)), main ="ACF(serie)", lwd=2)
pacf(d1d12lnserie, ylim=c(-1,1), lag.max=60, col=c(rep(1,11),2), main ="PACF(serie)", lwd=2)
```

* Under Stationarity: ACF falls immediately from 1 to 0
* Under Non-stationary: the ACF declines gradually from 1 to 0 over a prolonged period of time

Thus, we can conclude that `d1d12lnserie` is a stationary series.

Final transformation of the total production of renewable energy (USA) timeseries:
$W_t = (1 - B)(1 - B^{12})\log X_t$

## Analyze the ACF and PACF of the stationary series to identify at least two plausible models. 
Reason about what features of the correlograms you use to identify these models.

* Seasonal part (red lags): MA(Q=1) or AR(P=4). Since MA(Q=1) has only a single parameter it is preferred.
  * (0,1,1)_12 (MA(Q=1))
  * (4,1,0)_12 (AR(P=4))
* Regular part (black lags): MA(q=2) (considering that black lags around 2nd red lag to be satellites not taking them into account) or AR(p=3) (again considering that black lags around 2nd red lag to be satellites not taking them into account). Since AR(p=3) has 3 parameters we can try with ARMA(1,1) as well.
  * (0,1,2) (MA(q=2))
  * (3,1,0) (AR(p=3))
  * (1,1,1) (ARMA(p=1,q=1))

### Final 6 proposed models:
* (0,1,2)(0,1,1)_12
* (3,1,0)(0,1,1)_12
* (1,1,1)(0,1,1)_12
* (0,1,2)(4,1,0)_12
* (3,1,0)(4,1,0)_12
* (1,1,1)(4,1,0)_12

# Estimation

## Use R to estimate the identified models.

```{r}
plot(d1d12lnserie)
abline(h=0) # black at 0
abline(h=mean(d1d12lnserie), col=2) # red close to 0
```


```{r}
(mod0a=arima(d1d12lnserie,order=c(0,0,2),seasonal=list(order=c(0,0,1),period=12)))
```

```{r}
(mod0b=arima(d1d12lnserie,order=c(3,0,0),seasonal=list(order=c(0,0,1),period=12)))
```


```{r}
(mod0c=arima(d1d12lnserie,order=c(1,0,1),seasonal=list(order=c(0,0,1),period=12)))
```

```{r}
(mod0d=arima(d1d12lnserie,order=c(0,0,2),seasonal=list(order=c(4,0,0),period=12)))
```

```{r}
(mod0e=arima(d1d12lnserie,order=c(3,0,0),seasonal=list(order=c(4,0,0),period=12)))
```

```{r}
(mod0f=arima(d1d12lnserie,order=c(1,0,1),seasonal=list(order=c(4,0,0),period=12)))
```
```{r}
# abs(mod$coef/sqrt(diag(mod$var.coef)))
# cat("\nSignificant?:",abs(model$coef/sqrt(diag(model$var.coef)))>2)
```


In all cases, the t-ratio of the intercept is lower than 2, for example:

* `mod0a` and `mod0c`:

  * \[ \frac{1e-04}{2e-04} = 0.5 < 2 \]

The same is true for the rest of the models following the same idea, meaning that the intercept is not significant. Since the mean was very close to 0 the result was expected. Thus, we can discard the mean, meaning we can move to the training of the model with the original format of the data (after the application of the logarithm).

## Estimating parameters and checking their significance

```{r}
(mod1a <- arima(lnserie, order=c(0,1,2), seasonal=list(order=c(0,1,1), period=12)))
```
** AIC decreased when the intercept was dropped; thus good decision to drop the intercept!**

Let's check the significance of the parameters of `mod1a`:
ma1: \[
\frac{0.2486}{0.0537} = 4.629423 > 2
\]
ma2: \[
\frac{0.2256}{0.0558} = 4.043011 > 2
\]
sma1: \[
\frac{0.8317}{0.0437} = 19.03204 > 2
\]

Thus we can see that all parameters are significant, no need to fix any of them.

Let's check the significance of the parameters of `mod1b` now:

```{r}
(mod1b=arima(lnserie,order=c(3,1,0),seasonal=list(order=c(0,1,1),period=12)))
```
ar1: \[
\frac{0.2531}{0.0550} = 4.601818 > 2
\]
ar2: \[
\frac{0.2808}{0.0541} = 5.190388 > 2
\]
ar3: \[
\frac{0.0982}{0.0542} = 1.811808 < 2
\]
sma1: \[
\frac{0.8163}{0.0445} = 18.93409 > 2
\]

we can see that `ar3` is not significant, thus we can retrain the model by downgrading the AR part to have `p=2`.


```{r}
(mod1b_2=arima(lnserie,order=c(2,1,0),seasonal=list(order=c(0,1,1),period=12)))
```

ar1: \[
\frac{0.2287}{0.0535} = 4.274766 > 2
\]
ar2: \[
\frac{0.2581}{0.0528} = 4.888258 > 2
\]
sma1: \[
\frac{0.8123}{0.0439} = 18.50342 > 2
\]

Now all parameters are significant, athough the `AIC` measure got higher for approx. 1 value. 

Let's check the significance of the parameters of `mod1c` now:

```{r}
(mod1c <- arima(lnserie, order=c(1,1,1),seasonal=list(order=c(0,1,1),period=12)))
```
ar1: \[
\frac{0.4597}{0.1157} = 3.973207 > 2
\]
ma1: \[
\frac{0.7434}{0.0872} = 8.525229 > 2
\]
sma1: \[
\frac{0.8331}{0.0440} = 18.93409 > 2
\]

again all of them are significant.

Let's check the significance of the parameters of `mod1d` now:

```{r}
(mod1d=arima(lnserie,order=c(0,1,2),seasonal=list(order=c(4,1,0),period=12)))
```

ma1: \[
\frac{0.2537}{0.0548} = 4.629562 > 2
\]
ma2: \[
\frac{0.2135}{0.0550} = 3.881818 > 2
\]
sar1: \[
\frac{0.625}{0.054} = 11.57407 > 2
\]
sar2: \[
\frac{0.5160}{0.0627} = 8.229665 > 2
\]
sar3: \[
\frac{0.3151}{0.0627} = 5.025518 > 2
\]
sar4: \[
\frac{0.2303}{0.0557} = 4.13465 > 2
\]

For `mod1e`:

```{r}
(mod1e=arima(lnserie,order=c(3,1,0),seasonal=list(order=c(4,1,0),period=12)))
```

ar1: \[
\frac{0.2679}{0.0615} = 4.356098 > 2
\]
ar2: \[
\frac{0.2949}{0.0786} = 3.751908 > 2
\]
ar3: \[
\frac{0.0972}{0.0602} = 1.614618 < 2
\]
sar1: \[
\frac{0.6291}{0.0632} = 9.954114 > 2
\]
sar2: \[
\frac{0.5229}{0.0888} = 5.888514 > 2
\]
sar3: \[
\frac{0.3126}{0.0751} = 4.16245 > 2
\]
sar4: \[
\frac{0.2222}{0.0848} = 2.620283 > 2
\]

Again, it can be seen that `ar3` is an insignificant parameter, for that reason we will change it to an `AR` with `p=2`.

```{r}
(mod1e_2=arima(lnserie,order=c(2,1,0),seasonal=list(order=c(4,1,0),period=12)))
```

ar1: \[
\frac{0.2417}{0.0538} = 4.492565 > 2
\]
ar2: \[
\frac{0.2706}{0.0530} = 5.10566 > 2
\]
sar1: \[
\frac{0.6342}{0.0544} = 11.65809 > 2
\]
sar2: \[
\frac{0.5209}{0.0632} = 8.242089 > 2
\]
sar3: \[
\frac{0.3149}{0.0631} = 4.990491 > 2
\]
sar4: \[
\frac{0.2247}{0.0558} = 4.026882 > 2
\]

For `mod1f`:

```{r}
(mod1f=arima(lnserie,order=c(1,1,1),seasonal=list(order=c(4,1,0),period=12)))

```

Again:

ar1: \[
\frac{0.4016}{0.1315} = 3.053992 > 2
\]
ma1: \[
\frac{0.6974}{0.1033} = 6.75121 > 2
\]
sar1: \[
\frac{0.6255}{0.0542} = 11.54059 > 2
\]
sar2: \[
\frac{0.5100}{0.0630} = 8.095238 > 2
\]
sar3: \[
\frac{0.3100}{0.0628} = 4.936306 > 2
\]
sar4: \[
\frac{0.2269}{0.0558} = 4.066308 > 2
\]


To conclude, we can see that between all models the lowest AIC values is $AIC = -1230.79$ for `mod1a`, followed by `mod1b` with $AIC = -1228.93$. For this reason it is decided to continue the analysis with `mod1a` and `mod1b`.

# Validation

```{r}
#################Validation#################################
validation=function(model){
  s=frequency(get(model$series))
  resi=model$residuals
  par(mfrow=c(2,2),mar=c(3,3,3,3))
  #Residuals plot
  plot(resi,main="Residuals")
  abline(h=0)
  abline(h=c(-3*sd(resi),3*sd(resi)),lty=3,col=4)
  #Square Root of absolute values of residuals (Homocedasticity)
  scatter.smooth(sqrt(abs(resi)),main="Square Root of Absolute residuals",
                 lpars=list(col=2))
  
  #Normal plot of residuals
  qqnorm(resi)
  qqline(resi,col=2,lwd=2)
  
  ##Histogram of residuals with normal curve
  hist(resi,breaks=20,freq=FALSE)
  curve(dnorm(x,mean=mean(resi),sd=sd(resi)),col=2,add=T)
  
  
  #ACF & PACF of residuals
  par(mfrow=c(1,2))
  acf(resi,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=2)
  pacf(resi,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=2)
  par(mfrow=c(1,1))
  
  #Ljung-Box p-values
  par(mar=c(2,2,1,1))
  tsdiag(model,gof.lag=7*s)
  cat("\n--------------------------------------------------------------------\n")
  print(model)
  
  #Stationary and Invertible
  cat("\nModul of AR Characteristic polynomial Roots: ", 
      Mod(polyroot(c(1,-model$model$phi))),"\n")
  cat("\nModul of MA Characteristic polynomial Roots: ",
      Mod(polyroot(c(1,model$model$theta))),"\n")
  
  suppressMessages(require(forecast,quietly=TRUE,warn.conflicts=FALSE))
  plot(model)
  
  #Model expressed as an MA infinity (psi-weights)
  psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
  names(psis)=paste("psi",1:36)
  cat("\nPsi-weights (MA(inf))\n")
  cat("\n--------------------\n")
  print(psis[1:24])
  
  #Model expressed as an AR infinity (pi-weights)
  pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
  names(pis)=paste("pi",1:36)
  cat("\nPi-weights (AR(inf))\n")
  cat("\n--------------------\n")
  print(pis[1:24])
   
  cat("\nDescriptive Statistics for the Residuals\n")
  cat("\n----------------------------------------\n") 
  
  suppressMessages(require(fBasics,quietly=TRUE,warn.conflicts=FALSE))
  ##Anderson-Darling test
  print(basicStats(resi))
  
  ## Add here complementary tests (use with caution!)
  ##---------------------------------------------------------
  cat("\nNormality Tests\n")
  cat("\n--------------------\n")
 
  ##Shapiro-Wilks Normality test
  print(shapiro.test(resi))

  suppressMessages(require(nortest,quietly=TRUE,warn.conflicts=FALSE))
  ##Anderson-Darling test
  print(ad.test(resi))
  
  suppressMessages(require(tseries,quietly=TRUE,warn.conflicts=FALSE))
  ##Jarque-Bera test
  print(jarque.bera.test(resi))
  
  cat("\nHomoscedasticity Test\n")
  cat("\n--------------------\n")
  suppressMessages(require(lmtest,quietly=TRUE,warn.conflicts=FALSE))
  ##Breusch-Pagan test
  obs=get(model$series)
  print(bptest(resi~I(obs-resi)))
  
  cat("\nIndependence Tests\n")
  cat("\n--------------------\n")
  
  ##Durbin-Watson test
  print(dwtest(resi~I(1:length(resi))))
  
  ##Ljung-Box test
  cat("\nLjung-Box test\n")
  print(t(apply(matrix(c(1:4,(1:4)*s)),1,function(el) {
    te=Box.test(resi,type="Ljung-Box",lag=el)
    c(lag=(te$parameter),statistic=te$statistic[[1]],p.value=te$p.value)})))
  
}
################# Fi Validation #################################
```

## mod1a

### Residual Plot

In the following plot we need to see a constant variance of the residuals. Between the blue lines of -2, +2 stds (for each thousand only 2 can be outside) I need to find the 95% of the observations. Or 97.5% between -3, +3 stds (for each thousand only 3 can be outside).

```{r}
res = resid(mod1a)
plot(res, type='l')
abline(h=0)
abline(h=c(-3,-2,2,3)*sd(res), lty=3, col=4)
```

We can see that there are several cases where the data lay outside the intervals of standard deviations violating the afore-mentioned rule. Thus, either `clusters of volatility` exist, meaning that the variance increases (or in general changes a lot) in specific periods, or `outliers` are present which need to be treated. Outlier treatment is performed lately.


### Estimation of the Dispertion

```{r}
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Residuals plot
plot(res,main="Residuals")
abline(h=0)
abline(h=c(-3*sd(res),3*sd(res)),lty=3,col=4)
#Square Root of absolute values of residuals (Homocedasticity)
scatter.smooth(sqrt(abs(res)),main="Square Root of Absolute residuals",
lpars=list(col=2))
```

We can see in both graphical representations generated before, that the variance of the residuals is not constant. The red line shows that there is bigger variance during a specific period.

```{r}
par(mfrow=c(1,2))
acf(res^2, ylim=c(-1,1), lag.max = 72, col=c(2,rep(1,11)),lwd=2)
pacf(res^2, ylim=c(-1,1), lag.max = 72, col=c(rep(1,11),2), lwd=2)
par(mfrow=c(1,1))
```

When we find significant lags here, implies that there is volatility. It means that we have a time series that at some point the variability is small and at the same level in a different time the variability is high.

When heteroscedasticity is present, there are 2 things that might lead to this result, or we have outliers or volatility. If the points lying outside the borders are not so much most probably we are having outliers, otherwise we might have volatility.

** We need all the lags to be not significant, meaning that there is no correlation between the residuals with them-selfes **

### Normality test

```{r}
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Normal plot of residuals
qqnorm(res)
qqline(res,col=2,lwd=2)
##Histogram of residuals with normal curve
hist(res,breaks=10,freq=F)
curve(dnorm(x,mean=mean(res),sd=sd(res)),col=2,add=T)
```

We need to find the residuals lying near the red line. If we find curvature in the left plot, it represents asymmetry of the residuals. For example, in economic data we could find in the start and the end big differences from the red lines. If those points are a lot it might indicate volatility (HEAVY TAILS) (Excess of kurtosis) (this means that the kurtosis, 3rd moment, is greater than 3, that the probability of outliers is greater than the Gaussian), if they are not a lot, they could represent outliers. 

** if the number of points far from the reference line is quite small, then we can consider those points as outliers. For volatility, the number of points is gonna be much higher. **

** If we have dependence between the residuals, it means that the model cannot explain all the variance of the model, meaning that a more complex model might be necessary **

### Total Validation

```{r}
validation(mod1a)
```

If all the complex roots of the characteristic polynomial of the `MA` part lie outside the unit circle the model is invertible. Here, we deal with the inverse of the roots, so we want all the points inside instead of outside, which is true. Thus properties of the model are good, and we can work with finite memory in computer, however model was not validated because of outliers, some of residuals were not independent.

As for `ljung box` tests, we see several rejections of white noise for residuals, since there are p values lower than $0,05$. The null hypothesis for the Ljung-Box test is that the auto-correlations of a time series are zero for all lags up to a certain specified lag $k$.

** If p-value is below blue line (5%, significance level), it means I reject the null hypothesis for this specific lag. Meaning that those residuals do not come from a Normal distribution, meaning that they are dependent **

## mod1b

```{r}
validation(mod1b)
```

**ADD COMMENTS ON THE RESULTS OF mod1b HERE, ONCE THIS IS DONE WE NEED TO SELECT BETWEEN THE 2 MODELS (QUESTION 3D) AND THEN UPDATE THE CODE BELOW FOR QUESTION 4 AND 5. CODE OF QUESTION 4 IS READY, WE ONLY NEED TO CHANGE `pdq` and `PDQ` IN THE FOLLOWING COMMAND. OUTLIER TREATMENT IS COMPLETELY MISSING !!!**

* breushch pagan reject homoscedasticity
* ljung box too many lags below 0,05, reject white noise of combination of residuals. that combination of residuals come from with rnsoise. 
* no ma part, invertible? means all ar models are invertible, no root in ma part.
* causal because no ar part in roots.

# Predictions

## mod1a

```{r}
# capability of prediction and calculation of the measures
# position of last observation for train dataset is 12 2018. test from there until 2019 12. 
ultim=c(2018,12)
pdq=c(1,1,2)
PDQ=c(0,1,1)

serie2=window(serie, end=ultim)
lnserie2=log(serie2)
serie1=window(serie, end=ultim+c(1,0))
lnserie1=log(serie1)

(modA=arima(lnserie1,order=pdq,seasonal=list(order=PDQ,period=12)))

#we want to check if model with train dataset is very similar to model with whole set. this is to check if test set is stable. 
# we check magnitude, t ratio and signs.
# if they were not similar, model is not stable, and means i cannot calculate with metrics the model capability, structural change in test period. 


```

```{r}
(modB=arima(lnserie2,order=pdq,seasonal=list(order=PDQ,period=12)))
```


** Clearly, stability of the model is fulfilled! We observe similar results in terms of significance, sign and magnitud. In practice, this means that the correlation structure has not changed in the last year, and that the use of the complete series for making predictions is reliable.

Remark: Models adequacy can be compared via the AIC, but only when they are based on the same data and on the same scale. (For instance, you cannot compare the model based on the complete and incomplete series via the AIC!!) **


```{r}
pred=predict(modB,n.ahead=12)
pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)
se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)


ts.plot(serie,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-2,+2),type="o",main=paste("Model ARIMA(",paste(pdq,collapse=","),")(",paste(PDQ,collapse=","),")12",sep=""))
abline(v=(ultim[1]-2):(ultim[1]+2),lty=3,col=4)
```
```{r}
obs=window(serie,start=ultim+c(0,1))
pr=window(pr,start=ultim+c(0,1))
ts(data.frame(LowLim=tl[-1],Predic=pr,UpperLim=tu[-1],Observ=obs,Error=obs-pr,PercentError=(obs-pr)/obs),start=ultim+c(0,1),freq=12)
```
```{r}
mod.RMSE1=sqrt(sum((obs-pr)^2)/12)
mod.MAE1=sum(abs(obs-pr))/12
mod.RMSPE1=sqrt(sum(((obs-pr)/obs)^2)/12)
mod.MAPE1=sum(abs(obs-pr)/obs)/12

data.frame("RMSE"=mod.RMSE1,"MAE"=mod.MAE1,"RMSPE"=mod.RMSPE1,"MAPE"=mod.MAPE1)
```
```{r}
mCI1=mean(tu-tl)

cat("\nMean Length CI: ",mCI1)
```

```{r}
pred=predict(modA,n.ahead=12)
pr<-ts(c(tail(lnserie1,1),pred$pred),start=ultim+c(1,0),freq=12)
se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

tl1<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu1<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr1<-ts(exp(pr),start=ultim+c(1,0),freq=12)

ts.plot(serie,tl1,tu1,pr1,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=c(ultim[1]-2,ultim[1]+3),type="o",main=paste("Model ARIMA(",paste(pdq,collapse=","),")(",paste(PDQ,collapse=","),")12",sep=""))
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)
```
```{r}
(previs1=window(cbind(tl1,pr1,tu1),start=ultim+c(1,0)))
```

# Outlier Treatment

```{r}
########## Atípics (Outliers) ###############################################
source("atipics2.r")
```

* The atipics2 R-script contains two R-functions named outdetec and lineal.

* The outdetec R-function returns the automatically detected outliers and their types.

* The lineal R-function returns the so called linearized or theoretical series (free of outliers).

* An ARIMA model is identified and fitted to this linearized series. The found model, once validated, can be used for performing forecasting. It would be expected that one gets better predictions; particularly more precise ones. Be aware that this model might be different than the one found with no treatment of outliers.

## Outliers automatic detection and its treatment

```{r}
##Detection of outliers: In this case, we have applied a regular and a seasonal differentiation of order $S=12$. We set the criterion to $crit = 2.8$ and also the argument LS to TRUE.
## The crit value chosen by the researcher is typically fixed around 3; the LS argument is optional (= TRUE if one aims to detect a level shift)

mod.atip=outdetec(modA,dif=c(1,12),crit=2.8,LS=T) # automatic detection of outliers with crit=2.8 and LS =TRUE

#Estimated residual variance after outliers detection and treatment
mod.atip$sigma
```
Table with detected outliers, their types, magnitud, statistic values and chronology and percentage of variation (relative since in log scale)

```{r}
atipics=mod.atip$atip[order(mod.atip$atip[,1]),]
meses=c("Ene","Feb","Mar","Abr","May","Jun","Jul","Ago","Sep","Oct","Nov","Dic")

data.frame(atipics,Fecha=paste(meses[(atipics[,1]-1)%%12+1],start(lnserie)[1]+((atipics[,1]-1)%/%12)),perc.Obs=exp(atipics[,3])*100)
```

*Some interpretation of results:

The 14th observation is a transitory change (TC) type of outlier with a significant statistic’s value |6.12|>2. Its magnitud is given by Wcoeff=−0.21 in the log scale (our series was log-transformed), which means that a decrease in the number of passengers is observed with respect to what would have happened if this atypical had not taken place. The effect of the TC outlier is noticed in February 1991 (Irak war), but attenuates relatively fast after few periods (exponential decrease with delta=0.7, usually).

The second is an additive outlier (AO) that occurs in August 1992 (Olympic games in Bcn). As learned in theory, its effect is only noticed at that specific date.

In November 2012 a level shift (LS) type of outlier is detected; that coincides with the year of the second economic crisis. Its effect takes place from that moment on.

Be reminded that the magnitud column (W_coeff) is on the log scale), but if we appply the exponential function and multiply by 100% we get the percentage relative variation effect. In case of the previously mentioned TC in Feb. 1991, we get exp(−0.21594176)=0.8057822
. This means that, in Feb. 1991, we only observed 80% of the flights that would have occurred without the presence of this atypical phenomenon (Iraq war); an effect of a 19.37% decrease (black line below red). *

## Comparing observed series with linearized (without outliers) series
Plot together (in original scale) the observed and the linearized series (without outliers)

```{r}
lnserie.lin=lineal(lnserie,mod.atip$atip)
serie.lin=exp(lnserie.lin)

plot(serie.lin,col=2)
lines(serie)
```

#### Profile of outliers effect: plot of the outliers effect in the log-transformed series
```{r}
plot(lnserie-lnserie.lin)
```

## Identification and Estimation based on the Linearized Series
*Just by looking at this profile you can easily identify the different types of outliers…*

Identify an ARIMA model to the linearized series (in log-sacle in this case)
P(ACF) of linearized series (in log-scale)

```{r}
d1d12lnserie.lin=diff(diff(lnserie.lin,12))
par(mfrow=c(1,2))
acf(d1d12lnserie.lin,ylim=c(-1,1),lag.max=72,col=c(2,rep(1,11)),lwd=2)
pacf(d1d12lnserie.lin,ylim=c(-1,1),lag.max=72,col=c(rep(1,11),2),lwd=2)
par(mfrow=c(1,1))
```
* Seasonal part (red lags): MA(Q=1) or AR(P=4) again.
  * (0,1,1)_12 (MA(Q=1))
  * (4,1,0)_12 (AR(P=4))
* Regular part (black lags): MA(q=2), or MA(q=5) or AR(p=3). Since AR(p=3) has 3 parameters we can try with ARMA(1,1) as well. We could also check ARMA(2,1) or ARMA(1,2) as well, but for simplicity reasons we consider it future work.
  * (0,1,2) (MA(q=2))
  * (0,1,5) (MA(q=5))
  * (3,1,0) (AR(p=3))
  * (1,1,1) (ARMA(p=1,q=1))

### Final 8 proposed models:
* (0,1,2)(0,1,1)_12
* (0,1,5)(0,1,1)_12
* (3,1,0)(0,1,1)_12
* (1,1,1)(0,1,1)_12
* (0,1,2)(4,1,0)_12
* (0,1,5)(4,1,0)_12
* (3,1,0)(4,1,0)_12
* (1,1,1)(4,1,0)_12

```{r}
(mod.lin.a=arima(lnserie.lin,order=c(0,1,2),seasonal=list(order=c(0,1,1),period=12)))
```
```{r}
abs(mod.lin.a$coef/sqrt(diag(mod.lin.a$var.coef)))
cat("\nSignificant?:",abs(mod.lin.a$coef/sqrt(diag(mod.lin.a$var.coef)))>2)
```

All coeffs are significant! The residual variance decreased, as expected!

```{r}
(mod.lin.b=arima(lnserie.lin,order=c(0,1,5),seasonal=list(order=c(0,1,1),period=12)))
```
```{r}
abs(mod.lin.b$coef/sqrt(diag(mod.lin.b$var.coef)))
cat("\nSignificant?:",abs(mod.lin.b$coef/sqrt(diag(mod.lin.b$var.coef)))>2)
```
```{r}
(mod.lin.c=arima(lnserie.lin,order=c(3,1,0),seasonal=list(order=c(0,1,1),period=12)))
```
```{r}
abs(mod.lin.c$coef/sqrt(diag(mod.lin.c$var.coef)))
cat("\nSignificant?:",abs(mod.lin.c$coef/sqrt(diag(mod.lin.c$var.coef)))>2)
```
All coeffs are significant! The residual variance decreased even more, as expected!

```{r}
(mod.lin.d=arima(lnserie.lin,order=c(1,1,1),seasonal=list(order=c(0,1,1),period=12)))
```
```{r}
abs(mod.lin.d$coef/sqrt(diag(mod.lin.d$var.coef)))
cat("\nSignificant?:",abs(mod.lin.d$coef/sqrt(diag(mod.lin.d$var.coef)))>2)
```

```{r}
(mod.lin.e=arima(lnserie.lin,order=c(0,1,2),seasonal=list(order=c(4,1,0),period=12)))
```
```{r}
abs(mod.lin.e$coef/sqrt(diag(mod.lin.e$var.coef)))
cat("\nSignificant?:",abs(mod.lin.e$coef/sqrt(diag(mod.lin.e$var.coef)))>2)
```
```{r}
(mod.lin.f=arima(lnserie.lin,order=c(0,1,5),seasonal=list(order=c(4,1,0),period=12)))
```
```{r}
abs(mod.lin.f$coef/sqrt(diag(mod.lin.f$var.coef)))
cat("\nSignificant?:",abs(mod.lin.f$coef/sqrt(diag(mod.lin.f$var.coef)))>2)
```
```{r}
(mod.lin.g=arima(lnserie.lin,order=c(3,1,0),seasonal=list(order=c(4,1,0),period=12)))
```
```{r}
abs(mod.lin.g$coef/sqrt(diag(mod.lin.g$var.coef)))
cat("\nSignificant?:",abs(mod.lin.g$coef/sqrt(diag(mod.lin.g$var.coef)))>2)
```
```{r}
(mod.lin.h=arima(lnserie.lin,order=c(1,1,1),seasonal=list(order=c(4,1,0),period=12)))
```
```{r}
abs(mod.lin.h$coef/sqrt(diag(mod.lin.h$var.coef)))
cat("\nSignificant?:",abs(mod.lin.h$coef/sqrt(diag(mod.lin.h$var.coef)))>2)
```


Best AIC for `mod.lin.a`

Proceed to validate the model.

### Validation of model fitted to the (log) linearized series

```{r}
model=mod.lin.a       #Fitted ARIMA model: in this case mod is ARIMA(0,1,2)(0,1,1)_12 for lnserie
validation(model)
```




